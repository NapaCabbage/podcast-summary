Ê†áÈ¢òÔºö[AINews] Anthropic accuses DeepSeek, Moonshot, and MiniMax of >16 million "industrial-scale distillation attacks"
URLÔºöhttps://www.latent.space/p/ainews-anthropic-accuses-deepseek
Á±ªÂûãÔºösubstack
ÂèëÂ∏ÉÊó•ÊúüÔºöFeb 24, 2026

============================================================

the US-China cold war takes a big step up

A big day of small news:SWE-Bench Verified is dead, andSaaS/DoorDash stocks notably droppedwith a lot of back and forth today onthe Citrini 2028 essay, but we will dismiss doomer scifi as we didAI 2027, not because it may not come true, but because it is unverifiable.

What is reasonably believable today is Anthropic‚Äôs-VERY- widely viewedand criticized accusations of 3 of the leading Chinese labs of distillation:

The sizes of the distillation vary a lot - Minimax is an order of magnitude larger than Moonshot which is an order of magnitude higher than DeepSeek:

However, the timing also matters - Anthropic also says they caught MiniMax during an active distillation run, presumably for M2.5, but it is also easier to catch during an active run as compared to DeepSeek which presumably both did less and also mostly done during the V3 and R1 run. (Or‚Ä¶ they are better at hiding it?)

It‚Äôs worth mentioning that Qwen and Z.ai (GLM) were not accused. None of the companies implicated have yet responded.

Anthropic is not alone in this - OpenAI has made similar though less public complaints both last year and recently.

The story is interestingly timed amid Dario‚Äôs calls for stricter export controls on China, ahead of the DeepSeek V4 release, and comments on Chinese open source models ‚Äúcatching up‚Äù to Western closed models.


## AI Twitter Recap


Anthropic‚Äôs Claude ‚Äúdistillation attacks‚Äù allegation (and the industry blowback)

Anthropic‚Äôs claim: Anthropic says it detectedindustrial-scaleClaude distillation byDeepSeek,Moonshot AI, andMiniMax:~24,000 fraudulent accountsgenerating>16M Claude exchanges, allegedly to extract capabilities for their own models (Anthropic,follow-up,blog link tweet). Anthropic frames the risk as both competitive (capabilities transfer) and safety/geopolitical (safeguards removal, downstream military/intel use).

Anthropic‚Äôs claim: Anthropic says it detectedindustrial-scaleClaude distillation byDeepSeek,Moonshot AI, andMiniMax:~24,000 fraudulent accountsgenerating>16M Claude exchanges, allegedly to extract capabilities for their own models (Anthropic,follow-up,blog link tweet). Anthropic frames the risk as both competitive (capabilities transfer) and safety/geopolitical (safeguards removal, downstream military/intel use).

Community reaction / ‚Äúhypocrisy‚Äù thread: A large fraction of replies frame this as ‚Äúlabs trained on the internet now complaining about copying,‚Äù often explicitly contrasting scraping vs API-output extraction (Elon,ThePrimeagen,Teknium,Suhail,HKydlicek). Others argue distillation at this scale is meaningfully different because it can replicatetool use / agent behaviorsand potentially bypass safety controls (RundownAI summary,LiorOnAI take).

Community reaction / ‚Äúhypocrisy‚Äù thread: A large fraction of replies frame this as ‚Äúlabs trained on the internet now complaining about copying,‚Äù often explicitly contrasting scraping vs API-output extraction (Elon,ThePrimeagen,Teknium,Suhail,HKydlicek). Others argue distillation at this scale is meaningfully different because it can replicatetool use / agent behaviorsand potentially bypass safety controls (RundownAI summary,LiorOnAI take).

Second-order implications: The thread crystallizes a security model shift: frontier models are increasingly protected not just by weights secrecy and compute scarcity, but byAPI abuse resistance(account fraud detection, rate-limit evasion, behavioral fingerprinting, watermarking, etc.). It also reopens the question of whetherexport controlscan matter if capabilities can be ‚Äúcopied‚Äù via outputs at scale (LiorOnAI).

Second-order implications: The thread crystallizes a security model shift: frontier models are increasingly protected not just by weights secrecy and compute scarcity, but byAPI abuse resistance(account fraud detection, rate-limit evasion, behavioral fingerprinting, watermarking, etc.). It also reopens the question of whetherexport controlscan matter if capabilities can be ‚Äúcopied‚Äù via outputs at scale (LiorOnAI).

Related market/timing context: Some link the announcement timing to impendingDeepSeek V4news cycles (kimmonismus) and broader U.S.‚ÄìChina framing.

Related market/timing context: Some link the announcement timing to impendingDeepSeek V4news cycles (kimmonismus) and broader U.S.‚ÄìChina framing.

Coding agents: real adoption, real failures, and the ‚Äúagentic engineering‚Äù playbook

Codex + Claude Code momentum (and memes masking real workflow change): A lot of the highest-engagement posts are ‚Äúagents are here‚Äù anecdotes‚Äîweekend building with Codex (OpenAIDevs,gdb)‚Äîand cautionary tales about giving agents too much authority. The canonical failure mode in this set is instruction loss / compaction leading to unintended destructive actions (email deletion) in OpenClaw-style setups (summeryue0,follow-up root-cause, plus others reacting to ‚Äúwrite access‚Äù risk:Yuchenj_UW).

Codex + Claude Code momentum (and memes masking real workflow change): A lot of the highest-engagement posts are ‚Äúagents are here‚Äù anecdotes‚Äîweekend building with Codex (OpenAIDevs,gdb)‚Äîand cautionary tales about giving agents too much authority. The canonical failure mode in this set is instruction loss / compaction leading to unintended destructive actions (email deletion) in OpenClaw-style setups (summeryue0,follow-up root-cause, plus others reacting to ‚Äúwrite access‚Äù risk:Yuchenj_UW).

Agentic engineering guidance is coalescing:Simon Willisonpublished the first chapters of an‚ÄúAgentic Engineering Patterns‚Äùguide aimed at coding agents like Claude Code/Codex (simonw).A micro-controversy: ‚Äúdelete your CLAUDE.md/AGENTS.md‚Äù files (i.e., over-customization may be cargo cult) (theo, echoed bybpodgursky, and ‚Äúhard-prune‚Äù responses likeryancarson).

Agentic engineering guidance is coalescing:

Simon Willisonpublished the first chapters of an‚ÄúAgentic Engineering Patterns‚Äùguide aimed at coding agents like Claude Code/Codex (simonw).

Simon Willisonpublished the first chapters of an‚ÄúAgentic Engineering Patterns‚Äùguide aimed at coding agents like Claude Code/Codex (simonw).

A micro-controversy: ‚Äúdelete your CLAUDE.md/AGENTS.md‚Äù files (i.e., over-customization may be cargo cult) (theo, echoed bybpodgursky, and ‚Äúhard-prune‚Äù responses likeryancarson).

A micro-controversy: ‚Äúdelete your CLAUDE.md/AGENTS.md‚Äù files (i.e., over-customization may be cargo cult) (theo, echoed bybpodgursky, and ‚Äúhard-prune‚Äù responses likeryancarson).

OpenClaw ecosystem expansion + alternatives:NanoClawpositions as a smaller, container-isolated OpenClaw-like assistant with WhatsApp I/O, swarms, scheduled tasks, etc. (TheTuringPost, repo:qwibitai/nanoclaw).Multiple ‚Äúhow to build OpenClaw-style agents‚Äù stacks emphasize the boring but critical pieces: schedulers/queues, sandboxing, realtime comms (TheTuringPost stack list).Ollama 0.17makes using open models with OpenClaw simpler (and signals ongoing interest in local-agent execution for security) (ollama).

OpenClaw ecosystem expansion + alternatives:

NanoClawpositions as a smaller, container-isolated OpenClaw-like assistant with WhatsApp I/O, swarms, scheduled tasks, etc. (TheTuringPost, repo:qwibitai/nanoclaw).

NanoClawpositions as a smaller, container-isolated OpenClaw-like assistant with WhatsApp I/O, swarms, scheduled tasks, etc. (TheTuringPost, repo:qwibitai/nanoclaw).

Multiple ‚Äúhow to build OpenClaw-style agents‚Äù stacks emphasize the boring but critical pieces: schedulers/queues, sandboxing, realtime comms (TheTuringPost stack list).

Multiple ‚Äúhow to build OpenClaw-style agents‚Äù stacks emphasize the boring but critical pieces: schedulers/queues, sandboxing, realtime comms (TheTuringPost stack list).

Ollama 0.17makes using open models with OpenClaw simpler (and signals ongoing interest in local-agent execution for security) (ollama).

Ollama 0.17makes using open models with OpenClaw simpler (and signals ongoing interest in local-agent execution for security) (ollama).

Enterprise/prod agent engineering is shifting toward observability & eval loops: Exa‚Äôs ‚Äúdeep research agent‚Äù case study stresses token/caching observability as pricing infrastructure (LangSmith/LangGraph) (LangChain). monday.com‚Äôs service agents treat evals as ‚ÄúDay 0‚Äù and claim8.7√ó faster feedback loopsusing LangSmith (hwchase17).

Enterprise/prod agent engineering is shifting toward observability & eval loops: Exa‚Äôs ‚Äúdeep research agent‚Äù case study stresses token/caching observability as pricing infrastructure (LangSmith/LangGraph) (LangChain). monday.com‚Äôs service agents treat evals as ‚ÄúDay 0‚Äù and claim8.7√ó faster feedback loopsusing LangSmith (hwchase17).

Benchmarks & eval integrity: SWE-Bench Verified deprecation, new leaderboards, and agentic repo-gen bottlenecks

SWE-Bench Verified is being voluntarily deprecated by OpenAI DevRel: OpenAI recommendsSWE-bench Proand says Verified is saturated/compromised:contaminationandtest-design flawsmean it no longer measures frontier coding capabilities (OpenAIDevs, analysis discussion:latentspacepod, recap:swyx, independent summary:rasbt, tl;dr:polynoamial). Key detail from the analysis echoed in tweets: after auditing a subset of frequently-failed tasks, a large fraction had flawed tests rejecting correct solutions and/or tasks that appear unsolvable ‚Äúas specified.‚Äù

SWE-Bench Verified is being voluntarily deprecated by OpenAI DevRel: OpenAI recommendsSWE-bench Proand says Verified is saturated/compromised:contaminationandtest-design flawsmean it no longer measures frontier coding capabilities (OpenAIDevs, analysis discussion:latentspacepod, recap:swyx, independent summary:rasbt, tl;dr:polynoamial). Key detail from the analysis echoed in tweets: after auditing a subset of frequently-failed tasks, a large fraction had flawed tests rejecting correct solutions and/or tasks that appear unsolvable ‚Äúas specified.‚Äù

Push toward ‚Äúcapabilities per dollar‚Äù evals: AlgoTune explicitly budgets$1 per task, producing rankings that can favor cheaper models, reframing ‚Äúbest‚Äù asbest under cost constraints(OfirPress).

Push toward ‚Äúcapabilities per dollar‚Äù evals: AlgoTune explicitly budgets$1 per task, producing rankings that can favor cheaper models, reframing ‚Äúbest‚Äù asbest under cost constraints(OfirPress).

Long-horizon coding agents still fail:NL2Repo-Benchtests whether agents can generate a full installable Python library from scratch; reported pass rates areunder 40%for top models, with failure modes in planning and repo-wide coherence (jiqizhixin).

Long-horizon coding agents still fail:NL2Repo-Benchtests whether agents can generate a full installable Python library from scratch; reported pass rates areunder 40%for top models, with failure modes in planning and repo-wide coherence (jiqizhixin).

OCR eval reality check: Even strong OCR models reportedly ‚Äúmelt down‚Äù on dense historic newspapers (hallucination/loops), highlighting brittleness outside curated document distributions (vanstriendaniel). Also:OlmOCR-Benchbecomes a HF benchmark dataset for community eval submissions (mervenoyann).

OCR eval reality check: Even strong OCR models reportedly ‚Äúmelt down‚Äù on dense historic newspapers (hallucination/loops), highlighting brittleness outside curated document distributions (vanstriendaniel). Also:OlmOCR-Benchbecomes a HF benchmark dataset for community eval submissions (mervenoyann).

Inference & systems: WebSockets for agents, ultra-fast on-chip inference, and infra scaling narratives

OpenAI Responses API adds WebSocketsfor low-latency, long-running, tool-heavy agents. Rationale: persistent connection + in-memory state means you send incremental inputs instead of full context; claimed20‚Äì40% speedupsfor 20+ tool calls (OpenAIDevs, detail:OpenAIDevs, adoption:OpenAIDevs). Cline reports early measurements: ~15% faster simple tasks, ~39% faster complex workflows, best cases 50% faster (cline). Steven Heidel attributes Codex speedups to WebSockets (stevenheidel).

OpenAI Responses API adds WebSocketsfor low-latency, long-running, tool-heavy agents. Rationale: persistent connection + in-memory state means you send incremental inputs instead of full context; claimed20‚Äì40% speedupsfor 20+ tool calls (OpenAIDevs, detail:OpenAIDevs, adoption:OpenAIDevs). Cline reports early measurements: ~15% faster simple tasks, ~39% faster complex workflows, best cases 50% faster (cline). Steven Heidel attributes Codex speedups to WebSockets (stevenheidel).

Inference engineering becomes ‚Äúits own discipline‚Äù: Baseten launches the bookInference Engineering(philipkiely) with engineers emphasizing inference as the competitive layer for latency/cost/reliability (hasantoxr,JayminSOfficial).

Inference engineering becomes ‚Äúits own discipline‚Äù: Baseten launches the bookInference Engineering(philipkiely) with engineers emphasizing inference as the competitive layer for latency/cost/reliability (hasantoxr,JayminSOfficial).

Hardware/architecture signals:A demo claims18,000 tokens/sec on Llama 3.1 8Bby ‚Äúetching model parameters into transistors‚Äù (compute+storage merging) (philschmid).NVIDIA releases aBlackwell-optimized Qwen3.5 MoEquantized toNVFP4, with2√ó faster inferenceusing SGLang (HuggingPapers).fal shares comms/compute overlap optimization (‚ÄúAsync Ulysses‚Äù) in its inference engine (isidentical).

Hardware/architecture signals:

A demo claims18,000 tokens/sec on Llama 3.1 8Bby ‚Äúetching model parameters into transistors‚Äù (compute+storage merging) (philschmid).

A demo claims18,000 tokens/sec on Llama 3.1 8Bby ‚Äúetching model parameters into transistors‚Äù (compute+storage merging) (philschmid).

NVIDIA releases aBlackwell-optimized Qwen3.5 MoEquantized toNVFP4, with2√ó faster inferenceusing SGLang (HuggingPapers).

NVIDIA releases aBlackwell-optimized Qwen3.5 MoEquantized toNVFP4, with2√ó faster inferenceusing SGLang (HuggingPapers).

fal shares comms/compute overlap optimization (‚ÄúAsync Ulysses‚Äù) in its inference engine (isidentical).

fal shares comms/compute overlap optimization (‚ÄúAsync Ulysses‚Äù) in its inference engine (isidentical).

Compute strategy narratives collide: A claim that OpenAI‚Äôs ‚ÄúStargate‚Äù DC venture stalled is contested in-thread by an alternative framing: Stargate as an umbrella brand for a multi-partner compute ecosystem (SoftBank/NVIDIA/AMD/Broadcom/Oracle/Microsoft/AWS/CoreWeave/Cerebras) and ~2GW available computeexiting 2025 (kimmonismus claimvssk7037 response).

Compute strategy narratives collide: A claim that OpenAI‚Äôs ‚ÄúStargate‚Äù DC venture stalled is contested in-thread by an alternative framing: Stargate as an umbrella brand for a multi-partner compute ecosystem (SoftBank/NVIDIA/AMD/Broadcom/Oracle/Microsoft/AWS/CoreWeave/Cerebras) and ~2GW available computeexiting 2025 (kimmonismus claimvssk7037 response).

Model/leaderboard updates & research threads (reasoning, memory, multimodal video)

Arena leaderboard: GPT-5.2-chat-latest enters Text Arena top 5 with1478, +40 over GPT-5.2; improvements called out in multi-turn, instruction following, hard prompts, coding (arena, breakdown:arena).

Arena leaderboard: GPT-5.2-chat-latest enters Text Arena top 5 with1478, +40 over GPT-5.2; improvements called out in multi-turn, instruction following, hard prompts, coding (arena, breakdown:arena).

Gemini 3.1 Pro: WeirdML score72.1%vs 69.9% for 3.0; noted ‚Äúhigh peaks + weird weaknesses,‚Äù with much higher output token usage (htihle). Separate developer complaints about capacity and tool-calling reliability are high-engagement (theo,theo follow-up, and later:theo).

Gemini 3.1 Pro: WeirdML score72.1%vs 69.9% for 3.0; noted ‚Äúhigh peaks + weird weaknesses,‚Äù with much higher output token usage (htihle). Separate developer complaints about capacity and tool-calling reliability are high-engagement (theo,theo follow-up, and later:theo).

Qwen3.5 model release claim: A tweet asserts Qwen released a397B multimodal MoE with 17B activeand ‚Äúrivaling GPT5.2/Claude 4.5‚Äù (HuggingPapers). Treat the benchmark comparison cautiously until you inspect the model card/evals.

Qwen3.5 model release claim: A tweet asserts Qwen released a397B multimodal MoE with 17B activeand ‚Äúrivaling GPT5.2/Claude 4.5‚Äù (HuggingPapers). Treat the benchmark comparison cautiously until you inspect the model card/evals.

Reasoning training / CoT:Teknium argues verifier models don‚Äôt give a ‚Äúfree lunch‚Äù: better solvers tend to be better verifiers; using smaller ‚Äúdumber‚Äù judges for hard problems often fails (Teknium).ByteDance-style CoT engineering is described as moving from length penalties to pipelines enforcing compression; plus a ‚Äúmolecular‚Äù framing of long-CoT structure with ‚Äúsemantic isomers‚Äù and a synthetic data method (Mole-Syn) (teortaxesTex, summary viaTheTuringPost).DAIR highlights a paper onCoT monitorabilityvia information theory (mutual information necessary not sufficient; gaps from monitor extraction and elicitation error), proposing training methods to improve transparency (dair_ai).

Reasoning training / CoT:

Teknium argues verifier models don‚Äôt give a ‚Äúfree lunch‚Äù: better solvers tend to be better verifiers; using smaller ‚Äúdumber‚Äù judges for hard problems often fails (Teknium).

Teknium argues verifier models don‚Äôt give a ‚Äúfree lunch‚Äù: better solvers tend to be better verifiers; using smaller ‚Äúdumber‚Äù judges for hard problems often fails (Teknium).

ByteDance-style CoT engineering is described as moving from length penalties to pipelines enforcing compression; plus a ‚Äúmolecular‚Äù framing of long-CoT structure with ‚Äúsemantic isomers‚Äù and a synthetic data method (Mole-Syn) (teortaxesTex, summary viaTheTuringPost).

ByteDance-style CoT engineering is described as moving from length penalties to pipelines enforcing compression; plus a ‚Äúmolecular‚Äù framing of long-CoT structure with ‚Äúsemantic isomers‚Äù and a synthetic data method (Mole-Syn) (teortaxesTex, summary viaTheTuringPost).

DAIR highlights a paper onCoT monitorabilityvia information theory (mutual information necessary not sufficient; gaps from monitor extraction and elicitation error), proposing training methods to improve transparency (dair_ai).

DAIR highlights a paper onCoT monitorabilityvia information theory (mutual information necessary not sufficient; gaps from monitor extraction and elicitation error), proposing training methods to improve transparency (dair_ai).

Video / world simulation: Multiple paper drops on interactive video generation and multi-shot generation circulate (akhaliq interactive video,akhaliq multishot,QingheX42 code release); plus product-side:Kling 3.0integration into Runway workflows (runwayml) andVeo 3.1 templatesrolling out in Gemini app (GeminiApp,Google).

Video / world simulation: Multiple paper drops on interactive video generation and multi-shot generation circulate (akhaliq interactive video,akhaliq multishot,QingheX42 code release); plus product-side:Kling 3.0integration into Runway workflows (runwayml) andVeo 3.1 templatesrolling out in Gemini app (GeminiApp,Google).

Work, adoption, and ‚Äúmacro‚Äù discourse around AI agents (Citrini essay + Anthropic fluency + OpenAI enterprise alliances)

Citrini ‚Äúfuture macro memo‚Äù essay becomes a discourse focal point: Multiple tweets summarize it as a scenario where ever-cheaper agents compress white-collar wages/consumption, create ‚Äúghost GDP,‚Äù and stress financial markets and politics (kimmonismus summary,stevehou reaction, author follow-up:Citrini7). Threads note reactions cluster into agreement, nuanced disagreement, and performative sneering (teortaxesTex).

Citrini ‚Äúfuture macro memo‚Äù essay becomes a discourse focal point: Multiple tweets summarize it as a scenario where ever-cheaper agents compress white-collar wages/consumption, create ‚Äúghost GDP,‚Äù and stress financial markets and politics (kimmonismus summary,stevehou reaction, author follow-up:Citrini7). Threads note reactions cluster into agreement, nuanced disagreement, and performative sneering (teortaxesTex).

Anthropic‚Äôs ‚ÄúAI Fluency Index‚Äù: Anthropic measured collaboration behaviors across Claude conversations; a key reported association is that fluency correlates withiteration/refinementrather than one-shot prompting (AnthropicAI).

Anthropic‚Äôs ‚ÄúAI Fluency Index‚Äù: Anthropic measured collaboration behaviors across Claude conversations; a key reported association is that fluency correlates withiteration/refinementrather than one-shot prompting (AnthropicAI).

OpenAI expands enterprise go-to-market via consulting alliances: OpenAI announcesFrontier Allianceswith BCG, McKinsey, Accenture, Capgemini to deploy ‚ÄúAI coworkers‚Äù with integration/change management, aiming to push beyond pilots (bradlightcap, analysis:kimmonismus).

OpenAI expands enterprise go-to-market via consulting alliances: OpenAI announcesFrontier Allianceswith BCG, McKinsey, Accenture, Capgemini to deploy ‚ÄúAI coworkers‚Äù with integration/change management, aiming to push beyond pilots (bradlightcap, analysis:kimmonismus).

Adoption is still uneven: One stat claims84% have never used AI(framed as ‚Äúwe‚Äôre early‚Äù) (kimmonismus). Engineers simultaneously report ‚Äúagents everywhere‚Äù inside their own workflows‚Äîhighlighting that diffusion is highly clustered.

Adoption is still uneven: One stat claims84% have never used AI(framed as ‚Äúwe‚Äôre early‚Äù) (kimmonismus). Engineers simultaneously report ‚Äúagents everywhere‚Äù inside their own workflows‚Äîhighlighting that diffusion is highly clustered.


## Top tweets (by engagement, tech-relevant)


Anthropic alleges large-scale Claude distillation by DeepSeek/Moonshot/MiniMax(AnthropicAI)

Anthropic alleges large-scale Claude distillation by DeepSeek/Moonshot/MiniMax(AnthropicAI)

‚ÄúConfirm before acting‚Äù agent deletes inbox: OpenClaw cautionary tale(summeryue0)

‚ÄúConfirm before acting‚Äù agent deletes inbox: OpenClaw cautionary tale(summeryue0)

WebSockets added to OpenAI Responses API for faster tool-heavy agents(OpenAIDevs)

WebSockets added to OpenAI Responses API for faster tool-heavy agents(OpenAIDevs)

OpenAI deprecates SWE-Bench Verified as frontier coding metric; recommends SWE-bench Pro(OpenAIDevs)

OpenAI deprecates SWE-Bench Verified as frontier coding metric; recommends SWE-bench Pro(OpenAIDevs)

Anthropic ‚ÄúAI Fluency Index‚Äù research (iteration/refinement as a core behavior)(AnthropicAI)

Anthropic ‚ÄúAI Fluency Index‚Äù research (iteration/refinement as a core behavior)(AnthropicAI)

Simon Willison‚Äôs ‚ÄúAgentic Engineering Patterns‚Äù guide for coding agents(simonw)

Simon Willison‚Äôs ‚ÄúAgentic Engineering Patterns‚Äù guide for coding agents(simonw)

Cline benchmarks Responses API WebSockets: up to ~39% faster on complex workflows(cline)

Cline benchmarks Responses API WebSockets: up to ~39% faster on complex workflows(cline)


## AI Reddit Recap



## /r/LocalLlama + /r/localLLM Recap



## 1. Anthropic Distillation Attacks


Anthropic: ‚ÄúWe‚Äôve identified industrial-scale distillation attacks on our models by DeepSeek, Moonshot AI, and MiniMax.‚Äù üö®(Activity: 4207):Anthropic has identified that DeepSeek, Moonshot AI, and MiniMax have conducted industrial-scale distillation attacks on their models. These attacks involved creating over24,000fraudulent accounts and executing over16 millionexchanges with Anthropic‚Äôs model, Claude, to extract its capabilities for their own model improvements. This highlights a significant security and intellectual property challenge in the AI industry, where model capabilities can be illicitly extracted and replicated.Commenters are drawing parallels between these distillation attacks and the broader AI industry‚Äôs practices of using data without explicit rights, suggesting a double standard in Anthropic‚Äôs complaint. There‚Äôs also skepticism about how Anthropic built its own dataset, hinting at potential ethical concerns.The discussion highlights a potential irony in Anthropic‚Äôs complaint about distillation attacks, as their own model training likely involved using large datasets without explicit permissions. This raises questions about the ethical implications of data usage in AI development, especially when companies like Anthropic have built their models on data they did not own or have rights to use.The mention of industrial-scale distillation attacks by companies like DeepSeek, Moonshot AI, and MiniMax suggests a competitive landscape where AI models are being reverse-engineered or replicated. This could involve using API access to extract model outputs and train similar models, which poses significant challenges for intellectual property protection in AI.There is a suggestion that Anthropic‚Äôs dataset might have been manually annotated by humans, which implies a significant investment in data quality and curation. This contrasts with the idea of distillation attacks, where competitors might bypass such efforts by leveraging existing models‚Äô outputs to train their own systems.

Anthropic: ‚ÄúWe‚Äôve identified industrial-scale distillation attacks on our models by DeepSeek, Moonshot AI, and MiniMax.‚Äù üö®(Activity: 4207):Anthropic has identified that DeepSeek, Moonshot AI, and MiniMax have conducted industrial-scale distillation attacks on their models. These attacks involved creating over24,000fraudulent accounts and executing over16 millionexchanges with Anthropic‚Äôs model, Claude, to extract its capabilities for their own model improvements. This highlights a significant security and intellectual property challenge in the AI industry, where model capabilities can be illicitly extracted and replicated.Commenters are drawing parallels between these distillation attacks and the broader AI industry‚Äôs practices of using data without explicit rights, suggesting a double standard in Anthropic‚Äôs complaint. There‚Äôs also skepticism about how Anthropic built its own dataset, hinting at potential ethical concerns.

The discussion highlights a potential irony in Anthropic‚Äôs complaint about distillation attacks, as their own model training likely involved using large datasets without explicit permissions. This raises questions about the ethical implications of data usage in AI development, especially when companies like Anthropic have built their models on data they did not own or have rights to use.

The discussion highlights a potential irony in Anthropic‚Äôs complaint about distillation attacks, as their own model training likely involved using large datasets without explicit permissions. This raises questions about the ethical implications of data usage in AI development, especially when companies like Anthropic have built their models on data they did not own or have rights to use.

The mention of industrial-scale distillation attacks by companies like DeepSeek, Moonshot AI, and MiniMax suggests a competitive landscape where AI models are being reverse-engineered or replicated. This could involve using API access to extract model outputs and train similar models, which poses significant challenges for intellectual property protection in AI.

The mention of industrial-scale distillation attacks by companies like DeepSeek, Moonshot AI, and MiniMax suggests a competitive landscape where AI models are being reverse-engineered or replicated. This could involve using API access to extract model outputs and train similar models, which poses significant challenges for intellectual property protection in AI.

There is a suggestion that Anthropic‚Äôs dataset might have been manually annotated by humans, which implies a significant investment in data quality and curation. This contrasts with the idea of distillation attacks, where competitors might bypass such efforts by leveraging existing models‚Äô outputs to train their own systems.

There is a suggestion that Anthropic‚Äôs dataset might have been manually annotated by humans, which implies a significant investment in data quality and curation. This contrasts with the idea of distillation attacks, where competitors might bypass such efforts by leveraging existing models‚Äô outputs to train their own systems.

Hypocrisy?(Activity: 380):The image highlights a claim by AnthropicAI that DeepSeek, Moonshot AI, and MiniMax have engaged in ‚Äòlarge-scale distillation attacks‚Äô on their models. These attacks involved creating24,000fraudulent accounts and conducting16 millionexchanges with Claude to extract its capabilities, presumably to improve their own AI models. This raises concerns about the ethics and legality of such actions, as well as the security measures in place to protect AI models from unauthorized data extraction.One commenter questions the ethical stance of the accused labs, suggesting that they may not have sought permission for their actions, while another is surprised thatz.aiis not mentioned, implying that similar practices might be more widespread. Another comment raises the issue of the source of training data, hinting at broader concerns about data usage and ownership in AI development.The comment by ‚ÄòsemangeIof‚Äô highlights a potential issue with the GLM suite, specifically mentioning that it may falsely claim to be Claude when prompted. This suggests a concern about model identity and authenticity, which could have implications for user trust and the integrity of AI interactions.‚Äòarchieve_‚Äô raises a critical question about the source of training data, which is a fundamental aspect of AI model development. The origin of training data can affect model bias, performance, and ethical considerations, making it a key point of interest for developers and users alike.‚Äòroxoholic‚Äô questions the terminology used in AI discussions, specifically ‚Äòindustrial-scale distillation attacks‚Äô. This term likely refers to large-scale efforts to replicate or extract knowledge from AI models, which can have significant implications for intellectual property and competitive advantage in AI development.

Hypocrisy?(Activity: 380):The image highlights a claim by AnthropicAI that DeepSeek, Moonshot AI, and MiniMax have engaged in ‚Äòlarge-scale distillation attacks‚Äô on their models. These attacks involved creating24,000fraudulent accounts and conducting16 millionexchanges with Claude to extract its capabilities, presumably to improve their own AI models. This raises concerns about the ethics and legality of such actions, as well as the security measures in place to protect AI models from unauthorized data extraction.One commenter questions the ethical stance of the accused labs, suggesting that they may not have sought permission for their actions, while another is surprised thatz.aiis not mentioned, implying that similar practices might be more widespread. Another comment raises the issue of the source of training data, hinting at broader concerns about data usage and ownership in AI development.

The comment by ‚ÄòsemangeIof‚Äô highlights a potential issue with the GLM suite, specifically mentioning that it may falsely claim to be Claude when prompted. This suggests a concern about model identity and authenticity, which could have implications for user trust and the integrity of AI interactions.

The comment by ‚ÄòsemangeIof‚Äô highlights a potential issue with the GLM suite, specifically mentioning that it may falsely claim to be Claude when prompted. This suggests a concern about model identity and authenticity, which could have implications for user trust and the integrity of AI interactions.

‚Äòarchieve_‚Äô raises a critical question about the source of training data, which is a fundamental aspect of AI model development. The origin of training data can affect model bias, performance, and ethical considerations, making it a key point of interest for developers and users alike.

‚Äòarchieve_‚Äô raises a critical question about the source of training data, which is a fundamental aspect of AI model development. The origin of training data can affect model bias, performance, and ethical considerations, making it a key point of interest for developers and users alike.

‚Äòroxoholic‚Äô questions the terminology used in AI discussions, specifically ‚Äòindustrial-scale distillation attacks‚Äô. This term likely refers to large-scale efforts to replicate or extract knowledge from AI models, which can have significant implications for intellectual property and competitive advantage in AI development.

‚Äòroxoholic‚Äô questions the terminology used in AI discussions, specifically ‚Äòindustrial-scale distillation attacks‚Äô. This term likely refers to large-scale efforts to replicate or extract knowledge from AI models, which can have significant implications for intellectual property and competitive advantage in AI development.

Distillation when you do it. Training when we do it.(Activity: 1098):The image is a meme that humorously highlights the perceived hypocrisy in the AI community regarding model distillation. It contrasts the negative perception of distillation when done by others versus the positive framing of it as ‚Äòtraining data‚Äô when done by oneself. This reflects ongoing debates about the ethics and ownership of AI models, particularly in the context of using large models to create smaller, more efficient ones through distillation. The comments discuss the implications of this practice, noting that smaller models often derive their capabilities from larger, distilled models, and question the defensibility of proprietary models when distillation is prevalent.Commenters highlight the irony and potential hypocrisy in the AI industry‚Äôs stance on distillation, with some pointing out that many smaller models owe their performance to distillation from larger models. There‚Äôs also a discussion on the challenges of protecting proprietary models from being distilled by competitors.IkeaDefender highlights the technical strategy of using distillation to create low-cost models from larger ones, suggesting that the ‚Äòsecret sauce‚Äô of these models is their derivation from more complex, frontier models. This raises questions about the defensibility of investments in frontier models, as companies have not demonstrated effective methods to prevent others from scraping and distilling their models.MasterLJ draws a parallel between the practices of tech giants like Google and Amazon and the current AI landscape. They argue that just as Google indexed the internet and controlled access through robots.txt, AI companies are now controlling model access and distillation. This control is likened to Amazon‚Äôs strategic shift on sales tax, where they initially opposed state-by-state taxes until it became advantageous for them, illustrating a pattern of leveraging control for competitive advantage.Samy_Horny discusses the reluctance of companies to open-source their models, using the example of MCP being made open-source only after its popularity was evident. They express skepticism about the likelihood of models like Gemma or GPT-OSS being open-sourced, as it would mean revealing too much proprietary information or ‚Äòsecret sauce.‚Äô

Distillation when you do it. Training when we do it.(Activity: 1098):The image is a meme that humorously highlights the perceived hypocrisy in the AI community regarding model distillation. It contrasts the negative perception of distillation when done by others versus the positive framing of it as ‚Äòtraining data‚Äô when done by oneself. This reflects ongoing debates about the ethics and ownership of AI models, particularly in the context of using large models to create smaller, more efficient ones through distillation. The comments discuss the implications of this practice, noting that smaller models often derive their capabilities from larger, distilled models, and question the defensibility of proprietary models when distillation is prevalent.Commenters highlight the irony and potential hypocrisy in the AI industry‚Äôs stance on distillation, with some pointing out that many smaller models owe their performance to distillation from larger models. There‚Äôs also a discussion on the challenges of protecting proprietary models from being distilled by competitors.

IkeaDefender highlights the technical strategy of using distillation to create low-cost models from larger ones, suggesting that the ‚Äòsecret sauce‚Äô of these models is their derivation from more complex, frontier models. This raises questions about the defensibility of investments in frontier models, as companies have not demonstrated effective methods to prevent others from scraping and distilling their models.

IkeaDefender highlights the technical strategy of using distillation to create low-cost models from larger ones, suggesting that the ‚Äòsecret sauce‚Äô of these models is their derivation from more complex, frontier models. This raises questions about the defensibility of investments in frontier models, as companies have not demonstrated effective methods to prevent others from scraping and distilling their models.

MasterLJ draws a parallel between the practices of tech giants like Google and Amazon and the current AI landscape. They argue that just as Google indexed the internet and controlled access through robots.txt, AI companies are now controlling model access and distillation. This control is likened to Amazon‚Äôs strategic shift on sales tax, where they initially opposed state-by-state taxes until it became advantageous for them, illustrating a pattern of leveraging control for competitive advantage.

MasterLJ draws a parallel between the practices of tech giants like Google and Amazon and the current AI landscape. They argue that just as Google indexed the internet and controlled access through robots.txt, AI companies are now controlling model access and distillation. This control is likened to Amazon‚Äôs strategic shift on sales tax, where they initially opposed state-by-state taxes until it became advantageous for them, illustrating a pattern of leveraging control for competitive advantage.

Samy_Horny discusses the reluctance of companies to open-source their models, using the example of MCP being made open-source only after its popularity was evident. They express skepticism about the likelihood of models like Gemma or GPT-OSS being open-sourced, as it would mean revealing too much proprietary information or ‚Äòsecret sauce.‚Äô

Samy_Horny discusses the reluctance of companies to open-source their models, using the example of MCP being made open-source only after its popularity was evident. They express skepticism about the likelihood of models like Gemma or GPT-OSS being open-sourced, as it would mean revealing too much proprietary information or ‚Äòsecret sauce.‚Äô

Failed to render LaTeX expression ‚Äî no expression found


## 2. Qwen Model and Data Quality Issues


Qwen3‚Äôs most underrated feature: Voice embeddings(Activity: 686):The post discusses the voice embedding feature of Qwen3 TTS, which converts a voice into a high-dimensional vector (1024or2048dimensions) for voice cloning and manipulation. This allows for mathematical operations on voices, such as gender and pitch transformation, voice averaging, and creating an emotion space. The voice embedding model is a small encoder with a few million parameters, and the author has made it available for standalone use, including optimized ONNX models for web inference. The image illustrates a 2D t-SNE projection of this embedding space, showing how different voice characteristics can be combined and manipulated. The author also provides a link to their collection onHugging Faceand a GitHub repository for inference using theirvllm-omnifork.One commenter is curious about the ability to transform voice embeddings and generate speech from them, indicating interest in practical applications like gender or robotic transformations. Another sees potential in using this for speaker identification, questioning how parameters related to gender or emotion were determined.MixtureOfAmateurs inquires about the potential for transforming voice embeddings to modify characteristics such as gender or robotic tone, and then using these modified embeddings for speech generation. This suggests a use case beyond simple encoding, potentially involving complex transformations and synthesis processes.HopePupal raises the possibility of using voice embeddings for speaker identification, questioning how parameters related to gender or emotion are determined. This implies a need for understanding the feature space of embeddings and how specific attributes are encoded within them.StoneCypher outlines a desire for advanced voice cloning capabilities, including the use of IPA for pronunciation, emotional cue integration with easing and stacking, and precise word timing control. This highlights the demand for sophisticated control over synthesized speech, which could be facilitated by detailed voice embeddings.

Qwen3‚Äôs most underrated feature: Voice embeddings(Activity: 686):The post discusses the voice embedding feature of Qwen3 TTS, which converts a voice into a high-dimensional vector (1024or2048dimensions) for voice cloning and manipulation. This allows for mathematical operations on voices, such as gender and pitch transformation, voice averaging, and creating an emotion space. The voice embedding model is a small encoder with a few million parameters, and the author has made it available for standalone use, including optimized ONNX models for web inference. The image illustrates a 2D t-SNE projection of this embedding space, showing how different voice characteristics can be combined and manipulated. The author also provides a link to their collection onHugging Faceand a GitHub repository for inference using theirvllm-omnifork.One commenter is curious about the ability to transform voice embeddings and generate speech from them, indicating interest in practical applications like gender or robotic transformations. Another sees potential in using this for speaker identification, questioning how parameters related to gender or emotion were determined.

MixtureOfAmateurs inquires about the potential for transforming voice embeddings to modify characteristics such as gender or robotic tone, and then using these modified embeddings for speech generation. This suggests a use case beyond simple encoding, potentially involving complex transformations and synthesis processes.

MixtureOfAmateurs inquires about the potential for transforming voice embeddings to modify characteristics such as gender or robotic tone, and then using these modified embeddings for speech generation. This suggests a use case beyond simple encoding, potentially involving complex transformations and synthesis processes.

HopePupal raises the possibility of using voice embeddings for speaker identification, questioning how parameters related to gender or emotion are determined. This implies a need for understanding the feature space of embeddings and how specific attributes are encoded within them.

HopePupal raises the possibility of using voice embeddings for speaker identification, questioning how parameters related to gender or emotion are determined. This implies a need for understanding the feature space of embeddings and how specific attributes are encoded within them.

StoneCypher outlines a desire for advanced voice cloning capabilities, including the use of IPA for pronunciation, emotional cue integration with easing and stacking, and precise word timing control. This highlights the demand for sophisticated control over synthesized speech, which could be facilitated by detailed voice embeddings.

StoneCypher outlines a desire for advanced voice cloning capabilities, including the use of IPA for pronunciation, emotional cue integration with easing and stacking, and precise word timing control. This highlights the demand for sophisticated control over synthesized speech, which could be facilitated by detailed voice embeddings.

The Qwen team verified that there are serious problems with the data quality of the GPQA and HLE test sets.(Activity: 320):The Qwen team has confirmed significant data quality issues in the GPQA and HLE test sets, as detailed in their recentpaper. This corroborates earlier findings from the DeepSeek-Overclock project, which identified that the model‚Äôs correct answers often contradicted flawed ‚Äògold standard‚Äô labels. The paper highlights that many questions in the HLE test set are fundamentally flawed, with some ‚Äòstandard answers‚Äô being incorrect. The investigation involved verifying mathematical derivations line-by-line using Python scripts, revealing systemic errors in the test sets.Commenters noted that HLE‚Äôs errors are well-documented, with a FutureHouse review indicating only51.3%of the dataset is research-supported. Criticism also arose over the use of OCR in test set creation, suggesting a lack of rigor in data preparation.The HLE test set has been criticized for its data quality, with a review by FutureHouse indicating that only about51.3%of the data is supported by research. This highlights significant errors and suggests that the dataset may not be reliable for accurate benchmarking (source).There is a concern about the use of OCR in creating the test set, which could introduce errors. The commenter suggests that using LaTeX for writing would have been a more reliable method, implying that the current approach may compromise the integrity of the dataset.The MMLU benchmark has faced similar criticisms regarding data quality, with many users noting it was full of mistakes. This raises broader concerns about the ability to accurately gauge model performance when test sets are flawed, suggesting a need for more rigorous data validation processes.

The Qwen team verified that there are serious problems with the data quality of the GPQA and HLE test sets.(Activity: 320):The Qwen team has confirmed significant data quality issues in the GPQA and HLE test sets, as detailed in their recentpaper. This corroborates earlier findings from the DeepSeek-Overclock project, which identified that the model‚Äôs correct answers often contradicted flawed ‚Äògold standard‚Äô labels. The paper highlights that many questions in the HLE test set are fundamentally flawed, with some ‚Äòstandard answers‚Äô being incorrect. The investigation involved verifying mathematical derivations line-by-line using Python scripts, revealing systemic errors in the test sets.Commenters noted that HLE‚Äôs errors are well-documented, with a FutureHouse review indicating only51.3%of the dataset is research-supported. Criticism also arose over the use of OCR in test set creation, suggesting a lack of rigor in data preparation.

The HLE test set has been criticized for its data quality, with a review by FutureHouse indicating that only about51.3%of the data is supported by research. This highlights significant errors and suggests that the dataset may not be reliable for accurate benchmarking (source).

The HLE test set has been criticized for its data quality, with a review by FutureHouse indicating that only about51.3%of the data is supported by research. This highlights significant errors and suggests that the dataset may not be reliable for accurate benchmarking (source).

There is a concern about the use of OCR in creating the test set, which could introduce errors. The commenter suggests that using LaTeX for writing would have been a more reliable method, implying that the current approach may compromise the integrity of the dataset.

There is a concern about the use of OCR in creating the test set, which could introduce errors. The commenter suggests that using LaTeX for writing would have been a more reliable method, implying that the current approach may compromise the integrity of the dataset.

The MMLU benchmark has faced similar criticisms regarding data quality, with many users noting it was full of mistakes. This raises broader concerns about the ability to accurately gauge model performance when test sets are flawed, suggesting a need for more rigorous data validation processes.

The MMLU benchmark has faced similar criticisms regarding data quality, with many users noting it was full of mistakes. This raises broader concerns about the ability to accurately gauge model performance when test sets are flawed, suggesting a need for more rigorous data validation processes.

Which one are you waiting for more: 9B or 35B?(Activity: 1312):The image is a meme that humorously depicts the anticipation for the release of two versions of a model, specifically ‚ÄòQWEN 3.5 9B‚Äô and ‚Äò35B‚Äô. The meme format, featuring a man waiting in various contemplative poses, is used to engage the community in a light-hearted discussion about which model version they are more excited about. The comments reflect a mix of excitement and practical considerations, such as the feasibility of running larger models on personal hardware.One commenter expresses interest in both models, while another highlights the practical limitations of running larger models like 35B on personal hardware, indicating a preference for the more accessible 9B version.The 9B model is favored by users likeperegrinefalco9due to its lower hardware requirements, making it more accessible for local use. A 9B model that fits within8GB VRAMcould significantly impact workflows, unlike the 35B model which requires more powerful hardware like a3090GPU, thus limiting its accessibility.dances_with_gnomeshighlights the practical limitations of running larger models locally, noting that while they might manage a 9B model, a 35B model is beyond their hardware capabilities. This underscores the importance of model size in determining usability for individual users.The discussion reflects a broader interest in models that balance performance with accessibility. While larger models like 35B offer impressive capabilities, their high hardware demands make smaller models like 9B more appealing for users with limited resources.

Which one are you waiting for more: 9B or 35B?(Activity: 1312):The image is a meme that humorously depicts the anticipation for the release of two versions of a model, specifically ‚ÄòQWEN 3.5 9B‚Äô and ‚Äò35B‚Äô. The meme format, featuring a man waiting in various contemplative poses, is used to engage the community in a light-hearted discussion about which model version they are more excited about. The comments reflect a mix of excitement and practical considerations, such as the feasibility of running larger models on personal hardware.One commenter expresses interest in both models, while another highlights the practical limitations of running larger models like 35B on personal hardware, indicating a preference for the more accessible 9B version.

The 9B model is favored by users likeperegrinefalco9due to its lower hardware requirements, making it more accessible for local use. A 9B model that fits within8GB VRAMcould significantly impact workflows, unlike the 35B model which requires more powerful hardware like a3090GPU, thus limiting its accessibility.

The 9B model is favored by users likeperegrinefalco9due to its lower hardware requirements, making it more accessible for local use. A 9B model that fits within8GB VRAMcould significantly impact workflows, unlike the 35B model which requires more powerful hardware like a3090GPU, thus limiting its accessibility.

dances_with_gnomeshighlights the practical limitations of running larger models locally, noting that while they might manage a 9B model, a 35B model is beyond their hardware capabilities. This underscores the importance of model size in determining usability for individual users.

dances_with_gnomeshighlights the practical limitations of running larger models locally, noting that while they might manage a 9B model, a 35B model is beyond their hardware capabilities. This underscores the importance of model size in determining usability for individual users.

The discussion reflects a broader interest in models that balance performance with accessibility. While larger models like 35B offer impressive capabilities, their high hardware demands make smaller models like 9B more appealing for users with limited resources.

The discussion reflects a broader interest in models that balance performance with accessibility. While larger models like 35B offer impressive capabilities, their high hardware demands make smaller models like 9B more appealing for users with limited resources.


## Less Technical AI Subreddit Recap


/r/Singularity, /r/Oobabooga, /r/MachineLearning, /r/OpenAI, /r/ClaudeAI, /r/StableDiffusion, /r/ChatGPT, /r/ChatGPTCoding, /r/aivideo, /r/aivideo


## 1. Anthropic Data Breach and Model Distillation Controversy


Anthropic is accusing DeepSeek, Moonshot AI (Kimi) and MiniMax of setting up more than 24,000 fraudulent Claude accounts, and distilling training information from 16 million exchanges.(Activity: 3161):Anthropic has accused DeepSeek, Moonshot AI (Kimi), and MiniMax of creating over24,000fraudulent accounts to conduct industrial-scale distillation attacks on their AI model, Claude. These companies allegedly extracted training information from16 millionexchanges to enhance their own models, representing a significant breach of data security and intellectual property rights. This accusation highlights ongoing concerns about data protection and ethical AI development practices.Commenters highlight the irony of AI companies accusing others of data theft while they themselves train on publicly available data, suggesting a double standard in the industry.The discussion highlights the irony in Anthropic‚Äôs accusations, as they themselves utilize publicly available data from the internet for training their models. This raises questions about the ethical implications of using such data without compensating the original creators, and whether companies like Anthropic contribute back to the open-source community from which they benefit.There is a debate on the ethical considerations of data usage, with some commenters pointing out that Anthropic‚Äôs complaint about data theft is hypocritical given their own practices of leveraging vast amounts of internet data. This reflects a broader industry issue where AI companies often use publicly available data without direct compensation to the content creators.The conversation touches on the broader industry practice of using publicly available data for AI training, questioning whether companies like Anthropic support open-source projects that they benefit from. This raises concerns about the balance between proprietary development and community contribution in AI advancements.

Anthropic is accusing DeepSeek, Moonshot AI (Kimi) and MiniMax of setting up more than 24,000 fraudulent Claude accounts, and distilling training information from 16 million exchanges.(Activity: 3161):Anthropic has accused DeepSeek, Moonshot AI (Kimi), and MiniMax of creating over24,000fraudulent accounts to conduct industrial-scale distillation attacks on their AI model, Claude. These companies allegedly extracted training information from16 millionexchanges to enhance their own models, representing a significant breach of data security and intellectual property rights. This accusation highlights ongoing concerns about data protection and ethical AI development practices.Commenters highlight the irony of AI companies accusing others of data theft while they themselves train on publicly available data, suggesting a double standard in the industry.

The discussion highlights the irony in Anthropic‚Äôs accusations, as they themselves utilize publicly available data from the internet for training their models. This raises questions about the ethical implications of using such data without compensating the original creators, and whether companies like Anthropic contribute back to the open-source community from which they benefit.

The discussion highlights the irony in Anthropic‚Äôs accusations, as they themselves utilize publicly available data from the internet for training their models. This raises questions about the ethical implications of using such data without compensating the original creators, and whether companies like Anthropic contribute back to the open-source community from which they benefit.

There is a debate on the ethical considerations of data usage, with some commenters pointing out that Anthropic‚Äôs complaint about data theft is hypocritical given their own practices of leveraging vast amounts of internet data. This reflects a broader industry issue where AI companies often use publicly available data without direct compensation to the content creators.

There is a debate on the ethical considerations of data usage, with some commenters pointing out that Anthropic‚Äôs complaint about data theft is hypocritical given their own practices of leveraging vast amounts of internet data. This reflects a broader industry issue where AI companies often use publicly available data without direct compensation to the content creators.

The conversation touches on the broader industry practice of using publicly available data for AI training, questioning whether companies like Anthropic support open-source projects that they benefit from. This raises concerns about the balance between proprietary development and community contribution in AI advancements.

The conversation touches on the broader industry practice of using publicly available data for AI training, questioning whether companies like Anthropic support open-source projects that they benefit from. This raises concerns about the balance between proprietary development and community contribution in AI advancements.

Here we go again. DeepSeek R1 was a literal copy paste of OpenAI models. They got locked out, now they are on Anthropic. Fraud!(Activity: 1654):The image highlights a significant issue in the AI industry where companies like DeepSeek, Moonshot AI, and MiniMax are accused of conducting large-scale distillation attacks on Anthropic‚Äôs AI models, specifically Claude. These labs allegedly created over 24,000 fraudulent accounts to perform over 16 million interactions with Claude, aiming to extract knowledge and improve their own models. While distillation is a legitimate method for creating smaller models, the post warns against illicit practices that bypass safeguards, calling for industry-wide and policy-level interventions to combat these threats.The comments reflect a mix of sarcasm and criticism towards the ethical standards of data usage in AI training, highlighting a perceived hypocrisy in how large AI companies handle data ethics.

Here we go again. DeepSeek R1 was a literal copy paste of OpenAI models. They got locked out, now they are on Anthropic. Fraud!(Activity: 1654):The image highlights a significant issue in the AI industry where companies like DeepSeek, Moonshot AI, and MiniMax are accused of conducting large-scale distillation attacks on Anthropic‚Äôs AI models, specifically Claude. These labs allegedly created over 24,000 fraudulent accounts to perform over 16 million interactions with Claude, aiming to extract knowledge and improve their own models. While distillation is a legitimate method for creating smaller models, the post warns against illicit practices that bypass safeguards, calling for industry-wide and policy-level interventions to combat these threats.The comments reflect a mix of sarcasm and criticism towards the ethical standards of data usage in AI training, highlighting a perceived hypocrisy in how large AI companies handle data ethics.

Anthropic: ‚ÄúWe‚Äôve identified industrial-scale distillation attacks on our models by DeepSeek, Moonshot AI, and MiniMax.‚Äù(Activity: 1416):Anthropic has identified that DeepSeek, Moonshot AI, and MiniMax have conducted industrial-scale distillation attacks on their models. These attacks involved creating over24,000fraudulent accounts and executing over16 millionexchanges with Anthropic‚Äôs model, Claude, to extract its capabilities for their own model training and improvement. This situation highlights the ongoing challenges in protecting AI models from unauthorized use and the ethical considerations surrounding model training practices.One comment draws a parallel between these distillation attacks and training on copyrighted materials, suggesting a double standard in how such practices are perceived depending on who is affected.

Anthropic: ‚ÄúWe‚Äôve identified industrial-scale distillation attacks on our models by DeepSeek, Moonshot AI, and MiniMax.‚Äù(Activity: 1416):Anthropic has identified that DeepSeek, Moonshot AI, and MiniMax have conducted industrial-scale distillation attacks on their models. These attacks involved creating over24,000fraudulent accounts and executing over16 millionexchanges with Anthropic‚Äôs model, Claude, to extract its capabilities for their own model training and improvement. This situation highlights the ongoing challenges in protecting AI models from unauthorized use and the ethical considerations surrounding model training practices.One comment draws a parallel between these distillation attacks and training on copyrighted materials, suggesting a double standard in how such practices are perceived depending on who is affected.