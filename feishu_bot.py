"""
é£ä¹¦æœºå™¨äºº Webhook æœåŠ¡
æ¥æ”¶é£ä¹¦æ¶ˆæ¯ â†’ æ‰§è¡Œå‘½ä»¤æˆ–å¤„ç† URL â†’ å›å¤ç»“æœ

æ”¯æŒä¸¤ç±»æ¶ˆæ¯ï¼š
  1. æŒ‡ä»¤æ¶ˆæ¯ï¼šæ£€æŸ¥ / æŠ“å– / å¤„ç† / æ¥æº / çºªè¦ / å¸®åŠ©
  2. URL æ¶ˆæ¯ï¼šç›´æ¥å‘é€é“¾æ¥ï¼ˆè‡ªåŠ¨æŠ“å– + ç”Ÿæˆçºªè¦ + é‡å»ºç½‘é¡µï¼‰

éƒ¨ç½²æ–¹å¼ï¼š
  python feishu_bot.py          # å‰å°è¿è¡Œï¼ˆæµ‹è¯•ç”¨ï¼‰
  nohup python feishu_bot.py &  # åå°è¿è¡Œ

ç¯å¢ƒå˜é‡ï¼ˆæ”¾åœ¨ .env æ–‡ä»¶ä¸­ï¼‰ï¼š
  FEISHU_APP_ID         é£ä¹¦åº”ç”¨ App ID
  FEISHU_APP_SECRET     é£ä¹¦åº”ç”¨ App Secret
  FEISHU_VERIFY_TOKEN   äº‹ä»¶è®¢é˜…éªŒè¯ Token
  ARK_API_KEY           è±†åŒ… API Keyï¼ˆç”Ÿæˆçºªè¦æ—¶éœ€è¦ï¼‰

é£ä¹¦åå°é…ç½®ï¼š
  1. å¼€å‘è€…åå° â†’ åˆ›å»ºä¼ä¸šè‡ªå»ºåº”ç”¨
  2. èƒ½åŠ› â†’ æœºå™¨äºº
  3. äº‹ä»¶è®¢é˜… â†’ è¯·æ±‚åœ°å€ï¼šhttp://YOUR_ECS_IP:8080/feishu
  4. äº‹ä»¶è®¢é˜… â†’ æ·»åŠ äº‹ä»¶ï¼šim.message.receive_v1
  5. æƒé™ç®¡ç† â†’ å¼€é€šï¼šim:message + im:message:send_as_bot
  6. å‘å¸ƒåº”ç”¨
"""

import os
import re
import json
import subprocess
import threading
import sys
from http.server import HTTPServer, BaseHTTPRequestHandler
from urllib.request import urlopen, Request

# â”€â”€ é…ç½® â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
APP_DIR = os.path.dirname(os.path.abspath(__file__))
# ç¡®ä¿å·¥ä½œç›®å½•åœ¨ APP_DIRï¼Œä½¿ feed_monitor çš„ç›¸å¯¹è·¯å¾„ç”Ÿæ•ˆ
os.chdir(APP_DIR)
sys.path.insert(0, APP_DIR)

APP_ID       = os.environ.get('FEISHU_APP_ID', '')
APP_SECRET   = os.environ.get('FEISHU_APP_SECRET', '')
VERIFY_TOKEN = os.environ.get('FEISHU_VERIFY_TOKEN', '')

PYTHON = os.path.join(APP_DIR, '.venv', 'bin', 'python3')
if not os.path.exists(PYTHON):
    PYTHON = sys.executable

PORT = 8080

# URL æ£€æµ‹æ­£åˆ™
URL_RE = re.compile(r'https?://[^\s\u3000\uff0c\u3001\u3002\uff1f\uff01]+')

HELP_TEXT = """\
ğŸ¤– æ’­å®¢çºªè¦åŠ©æ‰‹

â”â” å‘é€é“¾æ¥ï¼Œè‡ªåŠ¨å¤„ç† â”â”
  ç›´æ¥å‘ URL         â†’ æŠ“å– + ç”Ÿæˆçºªè¦ + æ›´æ–°ç½‘é¡µ
  æ ‡é¢˜ï¼šxxx + URL    â†’ ç”¨æŒ‡å®šæ ‡é¢˜å¤„ç†
  åªæŠ“ URL           â†’ åªæŠ“åŸæ–‡ï¼Œä¸è°ƒç”¨ AI

  ç¤ºä¾‹ï¼š
    https://youtu.be/xxxxx
    æ ‡é¢˜ï¼šDario on AI Safety  https://youtu.be/xxxxx
    åªæŠ“ https://www.dwarkesh.com/p/episode

â”â” è®¢é˜…æ¥æºç®¡ç† â”â”
  æ£€æŸ¥    â€” åˆ—å‡ºæ–°é›†æ•°ï¼ˆä¸å¤„ç†ï¼‰
  æŠ“å–    â€” æŠ“å–æ‰€æœ‰æ–°é›†æ•°ï¼ˆä¸è°ƒç”¨ AIï¼‰
  å¤„ç†    â€” å®Œæ•´æµæ°´çº¿ï¼šæŠ“å– + ç”Ÿæˆçºªè¦ + é‡å»ºç½‘é¡µ
  æŠ“å– Lex Fridman  â†’ åªå¤„ç†è¯¥æ¥æº
  å¤„ç† Latent Space â†’ åªå¤„ç†è¯¥æ¥æº

â”â” æŸ¥è¯¢ â”â”
  æ¥æº    â€” åˆ—å‡ºå½“å‰è®¢é˜…æ¥æº
  çºªè¦    â€” åˆ—å‡ºå·²ç”Ÿæˆçš„çºªè¦
  å¸®åŠ©    â€” æ˜¾ç¤ºæœ¬è¯´æ˜
"""


# â”€â”€ é£ä¹¦ API â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def get_tenant_access_token():
    url = 'https://open.feishu.cn/open-apis/auth/v3/tenant_access_token/internal'
    body = json.dumps({'app_id': APP_ID, 'app_secret': APP_SECRET}).encode()
    req = Request(url, data=body, headers={'Content-Type': 'application/json'})
    with urlopen(req, timeout=10) as resp:
        data = json.loads(resp.read())
    return data.get('tenant_access_token', '')


def send_message(chat_id, text):
    token = get_tenant_access_token()
    url = 'https://open.feishu.cn/open-apis/im/v1/messages?receive_id_type=chat_id'
    body = json.dumps({
        'receive_id': chat_id,
        'msg_type': 'text',
        'content': json.dumps({'text': text}),
    }).encode()
    req = Request(url, data=body, headers={
        'Content-Type': 'application/json',
        'Authorization': f'Bearer {token}',
    })
    with urlopen(req, timeout=10) as resp:
        return json.loads(resp.read())


# â”€â”€ URL å¤„ç† â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def extract_url_and_title(text):
    """
    ä»æ¶ˆæ¯ä¸­æå– (url, title, scrape_only)ã€‚
    æ”¯æŒæ ¼å¼ï¼š
      https://...
      æ ‡é¢˜ï¼šxxx  https://...
      åªæŠ“ https://...
    è¿”å› (url, title, scrape_only)ï¼Œæ—  URL è¿”å› (None, '', False)
    """
    urls = URL_RE.findall(text)
    if not urls:
        return None, '', False

    url = urls[0]
    title = ''
    scrape_only = any(k in text for k in ['åªæŠ“', 'åªçˆ¬', 'scrape only', 'ä¸ç”Ÿæˆçºªè¦'])

    # æå–ç”¨æˆ·ç»™çš„æ ‡é¢˜
    for prefix in ['æ ‡é¢˜ï¼š', 'æ ‡é¢˜:', 'title:', 'Title:']:
        if prefix in text:
            after = text.split(prefix, 1)[1]
            # æ ‡é¢˜åˆ° URL æˆ–æ¢è¡Œä¸ºæ­¢
            candidate = re.split(r'https?://|\n', after)[0].strip()
            if candidate:
                title = candidate
            break

    return url, title, scrape_only


def fetch_url_title(url):
    """è‡ªåŠ¨ä» URL æå–æ ‡é¢˜ï¼ˆYouTube ç”¨ oEmbedï¼Œå…¶ä»–æŠ“ og:title / <title>ï¼‰"""
    try:
        if 'youtube.com' in url or 'youtu.be' in url:
            oembed = f'https://www.youtube.com/oembed?url={url}&format=json'
            req = Request(oembed, headers={'User-Agent': 'Mozilla/5.0'})
            with urlopen(req, timeout=10) as resp:
                data = json.loads(resp.read())
            return data.get('title', '')

        req = Request(url, headers={'User-Agent': 'Mozilla/5.0'})
        with urlopen(req, timeout=15) as resp:
            html = resp.read(80000).decode('utf-8', errors='ignore')

        # og:title ä¼˜å…ˆ
        m = re.search(r'<meta[^>]+property=["\']og:title["\'][^>]+content=["\'](.*?)["\']', html, re.I)
        if not m:
            m = re.search(r'<meta[^>]+content=["\'](.*?)["\'][^>]+property=["\']og:title["\']', html, re.I)
        if m:
            return m.group(1).strip()

        m = re.search(r'<title[^>]*>(.*?)</title>', html, re.I | re.S)
        if m:
            return m.group(1).strip()
    except Exception as e:
        print(f'[fetch_url_title] {e}')
    return ''


def run_url(url, title='', scrape_only=False):
    """
    å¤„ç†å•æ¡ URLï¼šæŠ“å– â†’ (å¯é€‰) ç”Ÿæˆçºªè¦ â†’ é‡å»ºç½‘é¡µã€‚
    è¿”å›å›å¤æ–‡æœ¬ã€‚
    """
    from feed_monitor import scrape_episode, detect_category, slugify

    env = {**os.environ, 'PYTHONUNBUFFERED': '1'}
    lines = []

    # â‘  è·å–æ ‡é¢˜
    if not title:
        print(f'[run_url] è‡ªåŠ¨è·å–æ ‡é¢˜ï¼š{url}')
        title = fetch_url_title(url)
    if not title:
        title = url.rstrip('/').split('/')[-1] or 'untitled'
    print(f'[run_url] æ ‡é¢˜={title!r}  scrape_only={scrape_only}')

    # â‘¡ æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
    slug = slugify(title)
    raw_path = os.path.join(APP_DIR, 'raw', f'{slug}.txt')
    summary_path = os.path.join(APP_DIR, 'summaries', f'{slug}.md')

    if os.path.exists(raw_path):
        lines.append(f'âš ï¸ åŸæ–‡å·²å­˜åœ¨ï¼šraw/{slug}.txt')
        if scrape_only:
            return '\n'.join(lines)
        # å·²æœ‰åŸæ–‡ä½†æ²¡æœ‰çºªè¦ï¼Œç»§ç»­ç”Ÿæˆçºªè¦
        if os.path.exists(summary_path):
            lines.append(f'âš ï¸ çºªè¦ä¹Ÿå·²å­˜åœ¨ï¼šsummaries/{slug}.mdï¼Œæ— éœ€é‡æ–°å¤„ç†ã€‚')
            return '\n'.join(lines)
    else:
        # â‘¢ æŠ“å–
        try:
            category = detect_category(title, 'å…¶ä»–')
            slug, char_count = scrape_episode(title, url, '', category)
            lines.append(f'âœ… æŠ“å–å®Œæˆï¼š{title}')
            lines.append(f'   {char_count:,} å­—ç¬¦ â†’ raw/{slug}.txt')
        except Exception as e:
            return f'âŒ æŠ“å–å¤±è´¥ï¼š{e}'

    if scrape_only:
        lines.append(f'\nå¦‚éœ€ç”Ÿæˆçºªè¦ï¼Œå‘é€ï¼šå¤„ç†çºªè¦ {slug}')
        return '\n'.join(lines)

    # â‘£ ç”Ÿæˆçºªè¦
    lines.append('æ­£åœ¨ç”Ÿæˆçºªè¦ï¼ˆçº¦ 1-2 åˆ†é’Ÿï¼‰...')
    # æå‰å‘ä¸€æ¡è¿›åº¦æ¶ˆæ¯ï¼ˆè°ƒç”¨æ–¹è´Ÿè´£å‘é€ï¼Œè¿™é‡Œåªè¿”å›ä¸­é—´çŠ¶æ€ä¸å‘é€ï¼‰
    result = subprocess.run(
        [PYTHON, 'auto_summarize.py', slug],
        capture_output=True, text=True,
        cwd=APP_DIR, env=env, timeout=300,
    )
    if result.returncode == 0:
        lines.append(f'âœ… çºªè¦å·²ç”Ÿæˆï¼šsummaries/{slug}.md')
    else:
        err = (result.stdout + result.stderr).strip()[:300]
        lines.append(f'âš ï¸ çºªè¦ç”Ÿæˆå¼‚å¸¸ï¼š{err}')

    # â‘¤ é‡å»ºç½‘é¡µ
    subprocess.run(
        [PYTHON, 'generator.py'],
        capture_output=True, text=True,
        cwd=APP_DIR, env=env, timeout=60,
    )
    lines.append('âœ… ç½‘é¡µå·²æ›´æ–°')

    return '\n'.join(lines)


# â”€â”€ æŒ‡ä»¤å¤„ç† â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def parse_command(text):
    """
    è§£ææŒ‡ä»¤æ¶ˆæ¯ï¼Œè¿”å› (cmd, source_name)
    cmd: 'dry-run' | 'scrape' | 'process' | 'sources' | 'summaries' | 'help'
         | 'make-summary'ï¼ˆåªå¯¹å·²æœ‰ raw ç”Ÿæˆçºªè¦ï¼‰
    """
    lower = text.lower()

    source = ''
    for prefix in ['å¤„ç† ', 'æŠ“å– ', 'process ', 'scrape ']:
        if lower.startswith(prefix):
            source = text[len(prefix):].strip()
            break

    # "å¤„ç†çºªè¦ <slug>" â†’ å¯¹å·²æœ‰ raw æ–‡ä»¶ç”Ÿæˆçºªè¦
    if lower.startswith('å¤„ç†çºªè¦ ') or lower.startswith('ç”Ÿæˆçºªè¦ '):
        slug = text.split(' ', 1)[1].strip()
        return 'make-summary', slug

    if any(k in lower for k in ['æ£€æŸ¥', 'dry-run', 'dry run', 'æœ‰ä»€ä¹ˆæ–°çš„', 'æœ‰æ–°çš„']):
        return 'dry-run', source
    if any(k in lower for k in ['åªæŠ“å–', 'æŠ“å–', 'scrape']):
        return 'scrape', source
    if any(k in lower for k in ['å¤„ç†', 'process', 'è¿è¡Œ', 'æ›´æ–°', 'æµæ°´çº¿']):
        return 'process', source
    if any(k in lower for k in ['æ¥æº', 'sources', 'é¢‘é“', 'åˆ—è¡¨']):
        return 'sources', ''
    if any(k in lower for k in ['çºªè¦', 'summaries', 'æœ‰å“ªäº›']):
        return 'summaries', ''
    return 'help', ''


def run_command(cmd, arg=''):
    """æ‰§è¡ŒæŒ‡ä»¤ï¼Œè¿”å›è¾“å‡ºæ–‡æœ¬"""
    env = {**os.environ, 'PYTHONUNBUFFERED': '1'}

    if cmd == 'dry-run':
        args = [PYTHON, 'feed_monitor.py', '--dry-run']
        if arg:
            args += ['--source', arg]

    elif cmd == 'scrape':
        args = [PYTHON, 'feed_monitor.py', '--scrape-only']
        if arg:
            args += ['--source', arg]

    elif cmd == 'process':
        args = [PYTHON, 'feed_monitor.py']
        if arg:
            args += ['--source', arg]

    elif cmd == 'make-summary':
        slug = arg
        if not slug:
            return 'ç”¨æ³•ï¼šå¤„ç†çºªè¦ <slug>'
        result = subprocess.run(
            [PYTHON, 'auto_summarize.py', slug],
            capture_output=True, text=True,
            cwd=APP_DIR, env=env, timeout=300,
        )
        subprocess.run([PYTHON, 'generator.py'], capture_output=True,
                       cwd=APP_DIR, env=env, timeout=60)
        out = (result.stdout + result.stderr).strip()
        return (out[:1800] + '\nâœ… ç½‘é¡µå·²æ›´æ–°') if result.returncode == 0 else f'âš ï¸ {out[:1800]}'

    elif cmd == 'sources':
        try:
            with open(os.path.join(APP_DIR, 'sources.yaml'), encoding='utf-8') as f:
                return f.read()[:2000]
        except Exception as e:
            return f'è¯»å–æ¥æºé…ç½®å¤±è´¥ï¼š{e}'

    elif cmd == 'summaries':
        summaries_dir = os.path.join(APP_DIR, 'summaries')
        try:
            files = sorted(f for f in os.listdir(summaries_dir) if f.endswith('.md'))
            return f'å…± {len(files)} ç¯‡çºªè¦ï¼š\n' + '\n'.join(files[:50])
        except Exception as e:
            return f'è¯»å–çºªè¦åˆ—è¡¨å¤±è´¥ï¼š{e}'

    else:
        return HELP_TEXT

    try:
        result = subprocess.run(
            args, capture_output=True, text=True,
            cwd=APP_DIR, env=env, timeout=300,
        )
        output = (result.stdout + result.stderr).strip()
        if len(output) > 2000:
            output = output[:1900] + '\nâ€¦ï¼ˆè¾“å‡ºå·²æˆªæ–­ï¼‰'
        return output or 'ï¼ˆæ— è¾“å‡ºï¼‰'
    except subprocess.TimeoutExpired:
        return 'â° å‘½ä»¤æ‰§è¡Œè¶…æ—¶ï¼ˆ5åˆ†é’Ÿï¼‰ï¼Œè¯·ç¨åæŸ¥çœ‹æ—¥å¿—ã€‚'
    except Exception as e:
        return f'æ‰§è¡Œå¤±è´¥ï¼š{e}'


# â”€â”€ æ¶ˆæ¯åˆ†å‘ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def handle_message_async(chat_id, text):
    """åå°çº¿ç¨‹ï¼šè§£ææ¶ˆæ¯ç±»å‹ï¼Œæ‰§è¡Œå¹¶å›å¤"""
    try:
        send_message(chat_id, 'â³ å¤„ç†ä¸­ï¼Œè¯·ç¨å€™...')
    except Exception:
        pass

    # ä¼˜å…ˆæ£€æµ‹ URL
    url, title, scrape_only = extract_url_and_title(text)
    if url:
        output = run_url(url, title, scrape_only)
    else:
        cmd, arg = parse_command(text)
        output = run_command(cmd, arg)

    try:
        send_message(chat_id, output)
    except Exception as e:
        print(f'[Error] å‘é€æ¶ˆæ¯å¤±è´¥ï¼š{e}')


# â”€â”€ HTTP æœåŠ¡ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#
# ç«¯ç‚¹æ€»è§ˆï¼š
#   POST /feishu              é£ä¹¦äº‹ä»¶ Webhookï¼ˆç»™é£ä¹¦å¹³å°ç”¨ï¼‰
#   GET  /api/check           æ£€æŸ¥æ–°é›†æ•°ï¼ˆdry-runï¼‰?source=å¯é€‰
#   GET  /api/sources         åˆ—å‡ºè®¢é˜…æ¥æº
#   GET  /api/summaries       åˆ—å‡ºå·²ç”Ÿæˆçºªè¦
#   POST /api/process         å®Œæ•´æµæ°´çº¿  {"source":"å¯é€‰"}
#   POST /api/scrape          åªæŠ“å–åŸæ–‡  {"source":"å¯é€‰"}
#   POST /api/url             å¤„ç†æŒ‡å®šé“¾æ¥ {"url":"...","title":"å¯é€‰","scrape_only":false}
#
# æ‰€æœ‰ /api/* è¿”å› JSONï¼š{"ok": true/false, "output": "æ–‡æœ¬ç»“æœ"}

def _api_response(ok, output):
    return json.dumps({'ok': ok, 'output': output}, ensure_ascii=False)


def _run_in_thread_and_wait(fn, *args, timeout=360):
    """åœ¨å­çº¿ç¨‹ä¸­è¿è¡Œ fn(*args)ï¼Œé˜»å¡ç­‰å¾…ç»“æœï¼ˆé¿å…é•¿ä»»åŠ¡å¡ä¸»çº¿ç¨‹æ± ï¼‰"""
    result = [None]
    exc    = [None]
    def target():
        try:
            result[0] = fn(*args)
        except Exception as e:
            exc[0] = e
    t = threading.Thread(target=target, daemon=True)
    t.start()
    t.join(timeout)
    if t.is_alive():
        return 'â° æ‰§è¡Œè¶…æ—¶ï¼Œä»»åŠ¡ä»åœ¨åå°è¿è¡Œï¼Œè¯·ç¨åæŸ¥çœ‹æ—¥å¿—ã€‚'
    if exc[0]:
        return f'âŒ æ‰§è¡Œå¼‚å¸¸ï¼š{exc[0]}'
    return result[0]


class FeishuHandler(BaseHTTPRequestHandler):

    # â”€â”€ GET /api/* â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def do_GET(self):
        from urllib.parse import urlparse, parse_qs
        parsed = urlparse(self.path)
        params = parse_qs(parsed.query)

        def qp(key):
            vals = params.get(key, [''])
            return vals[0].strip() if vals else ''

        if parsed.path == '/api/check':
            source = qp('source')
            output = run_command('dry-run', source)
            self._reply(200, _api_response(True, output))

        elif parsed.path == '/api/sources':
            output = run_command('sources')
            self._reply(200, _api_response(True, output))

        elif parsed.path == '/api/summaries':
            output = run_command('summaries')
            self._reply(200, _api_response(True, output))

        else:
            self._reply(404, _api_response(False, 'Unknown endpoint'))

    # â”€â”€ POST /feishu  +  POST /api/* â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def do_POST(self):
        length = int(self.headers.get('Content-Length', 0))
        raw_body = self.rfile.read(length)

        # â”€â”€ /api/* è·¯ç”± â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if self.path.startswith('/api/'):
            try:
                body = json.loads(raw_body) if raw_body else {}
            except Exception:
                body = {}

            if self.path == '/api/process':
                source = body.get('source', '')
                output = _run_in_thread_and_wait(run_command, 'process', source)
                self._reply(200, _api_response(True, output))

            elif self.path == '/api/scrape':
                source = body.get('source', '')
                output = _run_in_thread_and_wait(run_command, 'scrape', source)
                self._reply(200, _api_response(True, output))

            elif self.path == '/api/url':
                url        = body.get('url', '')
                title      = body.get('title', '')
                scrape_only = body.get('scrape_only', False)
                if not url:
                    self._reply(400, _api_response(False, 'ç¼ºå°‘ url å‚æ•°'))
                    return
                output = _run_in_thread_and_wait(run_url, url, title, scrape_only)
                self._reply(200, _api_response(True, output))

            else:
                self._reply(404, _api_response(False, 'Unknown endpoint'))
            return

        # â”€â”€ /feishu é£ä¹¦ Webhook â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if self.path != '/feishu':
            self._reply(404, 'Not found')
            return

        length = int(self.headers.get('Content-Length', 0))
        body = self.rfile.read(length)

        try:
            data = json.loads(body)
        except json.JSONDecodeError:
            self._reply(400, 'Bad JSON')
            return

        # é£ä¹¦ URL éªŒè¯æ¡æ‰‹
        if data.get('type') == 'url_verification':
            challenge = data.get('challenge', '')
            self._reply(200, json.dumps({'challenge': challenge}))
            return

        # å¤„ç†æ¶ˆæ¯äº‹ä»¶
        event = data.get('event', {})
        msg = event.get('message', {})
        sender = event.get('sender', {})

        # å¿½ç•¥ bot è‡ªå·±å‘çš„æ¶ˆæ¯
        if sender.get('sender_type') == 'app':
            self._reply(200, 'ok')
            return

        chat_id = msg.get('chat_id', '')
        try:
            content = json.loads(msg.get('content', '{}'))
        except Exception:
            content = {}
        text = content.get('text', '').strip()

        # å»æ‰é£ä¹¦è‡ªåŠ¨åŠ çš„ @mention å‰ç¼€
        if text.startswith('@'):
            text = ' '.join(text.split()[1:]).strip()

        if chat_id and text:
            threading.Thread(
                target=handle_message_async,
                args=(chat_id, text),
                daemon=True,
            ).start()

        self._reply(200, 'ok')

    def _reply(self, code, body):
        if isinstance(body, str):
            body = body.encode()
        self.send_response(code)
        self.send_header('Content-Type', 'application/json')
        self.send_header('Content-Length', len(body))
        self.end_headers()
        self.wfile.write(body)

    def log_message(self, fmt, *args):
        print(f'[{self.address_string()}] ' + fmt % args)


# â”€â”€ å…¥å£ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

if __name__ == '__main__':
    if not APP_ID or not APP_SECRET:
        print('[è­¦å‘Š] FEISHU_APP_ID / FEISHU_APP_SECRET æœªè®¾ç½®ï¼Œå°†æ— æ³•ä¸»åŠ¨å‘é€æ¶ˆæ¯ã€‚')
    print(f'é£ä¹¦ Bot ç›‘å¬ :{PORT}/feishu  å·¥ä½œç›®å½•={APP_DIR}')
    server = HTTPServer(('0.0.0.0', PORT), FeishuHandler)
    try:
        server.serve_forever()
    except KeyboardInterrupt:
        print('\nå·²åœæ­¢ã€‚')
